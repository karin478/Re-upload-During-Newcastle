{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demonstration.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnn9zOdY0oQ_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/gdrive/MyDrive/enron11.zip' -d '/content'"
      ],
      "metadata": {
        "id": "3PIPGN-L1Lrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "qbm3KC0A0ufM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"============check cpu config================\")\n",
        "\n",
        "!cat /proc/cpuinfo | grep model\\ name\n",
        "print(\"=============check ram config===============\")\n",
        "\n",
        "!cat /proc/meminfo | grep MemTotal"
      ],
      "metadata": {
        "id": "sr5ASFkr0xga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data processing\n",
        "\n",
        "\n",
        "import random\n",
        "import tensorflow as tf\n",
        "seed = 1\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "# importing system libraries\n",
        "\n",
        "from os import walk\n",
        "from string import punctuation\n",
        "from random import shuffle\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# importing additional libraries\n",
        "\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import nltk\n",
        "import pytest\n",
        "nltk.download('stopwords')\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "from nltk.test.classify_fixt import setup_module\n",
        "setup_module()\n",
        "from os import walk\n",
        "from string import punctuation\n",
        "from random import shuffle\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# importing additional libraries\n",
        "\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import nltk\n",
        "import pytest\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from nltk.test.classify_fixt import setup_module\n",
        "setup_module()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import collections\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "import keras\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read the whole data from the Enron Dataset into a variable allData.\n",
        "\n",
        "pathwalk = walk(r\"/content/enron1/\")\n",
        "\n",
        "allHamData, allSpamData = [], []\n",
        "for root, dr, file in pathwalk:\n",
        "    if 'ham' in str(file):\n",
        "        for obj in file:\n",
        "            with open(root + '/' + obj, encoding='latin1') as ip:\n",
        "                allHamData.append(\" \".join(ip.readlines()))\n",
        "                \n",
        "    elif 'spam' in str(file):\n",
        "        for obj in file:\n",
        "            with open(root + '/' + obj, encoding='latin1') as ip:\n",
        "                allSpamData.append(\" \".join(ip.readlines()))\n",
        "                \n",
        "                \n",
        "                \n",
        "\n",
        "allHamData = list(set(allHamData))\n",
        "allSpamData = list(set(allSpamData))\n",
        "\n",
        "\n",
        "hamPlusSpamData = allHamData + allSpamData\n",
        "labels = [\"ham\"]*len(allHamData) + [\"spam\"]*len(allSpamData)\n",
        "\n",
        "raw_df = pd.DataFrame({\"email\": hamPlusSpamData, \n",
        "                       \"label\": labels})\n",
        "\n",
        "\n",
        "\n",
        "raw_df.sample(5)\n",
        "raw_df['email']\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "\n",
        "def preprocess(data):\n",
        "    # tokenization\n",
        "    tokens = nltk.word_tokenize(data)\n",
        "    tokens = [w.lower() for w in tokens if w.isalpha()]\n",
        "\n",
        "    # finding uncommon words\n",
        "    cnt = Counter(tokens)\n",
        "    uncommons = cnt.most_common()[:-int(len(cnt)*0.1):-1]\n",
        "    \n",
        "    # listing stopwords from NLTK\n",
        "    stops = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "    # removing stop words and uncommon words\n",
        "    tokens = [w for w in tokens if (w not in stops and w not in uncommons)]\n",
        "\n",
        "    # lemmatization\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(w, pos='a') for w in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "label_encoder = sk.preprocessing.LabelEncoder()\n",
        "raw_df['label'] = label_encoder.fit_transform(raw_df.label)\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# deleting stopwords like a the this that\n",
        "def process_text(text):\n",
        "    no_punc = [char for char in text if char not in string.punctuation]\n",
        "    no_punc = ''.join(no_punc)\n",
        "    return ' '.join([word for word in no_punc.split() if word.lower() not in stopwords.words('english')])\n",
        "\n",
        "raw_df['email']=raw_df['email'].apply(process_text)\n",
        "raw_df['email']\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "def stemming (text):\n",
        "    return ''.join([stemmer.stem(word) for word in text])\n",
        "raw_df['email']=raw_df['email'].apply(stemming)\n",
        "raw_df.head()\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer= CountVectorizer()\n",
        "message_bow = vectorizer.fit_transform(raw_df['email'])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(message_bow,raw_df['label'],test_size=0.25)\n",
        "\n",
        "X = message_bow\n",
        "y = raw_df['label']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer = TfidfTransformer().fit(message_bow)\n",
        "\n",
        "# Transform entire BoW into tf-idf corpus\n",
        "message_bow_tfidf = tfidf_transformer.transform(message_bow)\n",
        "\n",
        "X_tfidf = message_bow_tfidf\n",
        "y = raw_df['label']\n",
        "X_tfidf_train,X_tfidf_test,y_tfidf_train,y_tfidf_test = train_test_split(message_bow_tfidf,raw_df['label'],test_size=0.25)\n",
        "\n",
        "\n",
        "\n",
        "texts1 = np.asarray(message_bow)\n",
        "labels1 = np.asarray(raw_df['label'])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ZwXUAPO0zxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Naive Bayesian\n",
        "#Support Vector Machine\n",
        "#Random Forest\n",
        "\n",
        "\n",
        "\n",
        "print(\"this is naive_bayes part\")\n",
        "print(\"=======================================================================================================\")\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb= MultinomialNB()\n",
        "nb.fit(X_train,y_train)\n",
        "y_pred = (nb.predict(X_test) > 0.5).astype(\"int32\")\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_pred)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "nb3= MultinomialNB()\n",
        "accuracies_nb_accuracy = cross_val_score(estimator=nb3, scoring=\"accuracy\", \n",
        "    X=X, y=y, cv=5)\n",
        "accuracies_nb_f1 = cross_val_score(estimator=nb3, scoring=\"f1\", \n",
        "    X=X, y=y, cv=5)\n",
        "accuracies_nb_recall = cross_val_score(estimator=nb3, scoring=\"recall\", \n",
        "    X=X, y=y, cv=5)\n",
        "accuracies_nb_precision = cross_val_score(estimator=nb3, scoring=\"precision\", \n",
        "    X=X, y=y, cv=5)\n",
        "\n",
        "print(\"NB corss validation :  %0.4f accuracy  with a standard deviation of %0.4f\" % (accuracies_nb_accuracy.mean(), accuracies_nb_accuracy.std()))\n",
        "print(\"NB corss validation :  %0.4f f1     with a standard deviation of %0.4f\" % (accuracies_nb_f1.mean(), accuracies_nb_f1.std()))\n",
        "print(\"NB corss validation :  %0.4f recall   with a standard deviation of %0.4f\" % (accuracies_nb_recall.mean(), accuracies_nb_recall.std()))\n",
        "print(\"NB corss validation :  %0.4f precision with a standard deviation of %0.4f\" % (accuracies_nb_precision.mean(), accuracies_nb_precision.std()))\n",
        "\n",
        "\n",
        "\n",
        "print(\"this is svm part\")\n",
        "print(\"=======================================================================================================\")\n",
        "\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "svmc3= svm.SVC()\n",
        "accuracies_svmc_accuracy = cross_val_score(estimator=svmc3, scoring=\"accuracy\", \n",
        "    X=X_tfidf, y=y, cv=5)\n",
        "accuracies_svmc_f1 = cross_val_score(estimator=svmc3, scoring=\"f1\", \n",
        "    X=X_tfidf, y=y, cv=5)\n",
        "accuracies_svmc_recall = cross_val_score(estimator=svmc3, scoring=\"recall\", \n",
        "    X=X_tfidf, y=y, cv=5)\n",
        "accuracies_svmc_precision = cross_val_score(estimator=svmc3, scoring=\"precision\", \n",
        "    X=X_tfidf, y=y, cv=5)\n",
        "\n",
        "print(\"SVM corss validation :  %0.4f accuracy  with a standard deviation of %0.4f\" % (accuracies_svmc_accuracy.mean(), accuracies_svmc_accuracy.std()))\n",
        "print(\"SVM corss validation :  %0.4f f1     with a standard deviation of %0.4f\" % (accuracies_svmc_f1.mean(), accuracies_svmc_f1.std()))\n",
        "print(\"SVM corss validation :  %0.4f recall   with a standard deviation of %0.4f\" % (accuracies_svmc_recall.mean(), accuracies_svmc_recall.std()))\n",
        "print(\"SVM corss validation :  %0.4f precision with a standard deviation of %0.4f\" % (accuracies_svmc_precision.mean(), accuracies_svmc_precision.std()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"this is Random Forest part\")\n",
        "print(\"=======================================================================================================\")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "R_tree = RandomForestClassifier(n_estimators = 1000, min_samples_split = 10, max_depth=100, random_state = 42)\n",
        "R_tree.fit(X_tfidf_train, y_tfidf_train)\n",
        "\n",
        "\n",
        "\n",
        "y_pred = R_tree.predict(X_tfidf_test)\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "R_tree3 = RandomForestClassifier(n_estimators = 1000, min_samples_split = 10, max_depth=100, random_state = 42)\n",
        "accuracies_rtree_accuracy = cross_val_score(estimator=R_tree3, scoring=\"accuracy\", \n",
        "    X=X_tfidf, y=y, cv=10)\n",
        "accuracies_rtree_f1 = cross_val_score(estimator=R_tree3, scoring=\"f1\", \n",
        "    X=X_tfidf, y=y, cv=10)\n",
        "accuracies_rtree_recall = cross_val_score(estimator=R_tree3, scoring=\"recall\", \n",
        "    X=X_tfidf, y=y, cv=10)\n",
        "accuracies_rtree_precision = cross_val_score(estimator=R_tree3, scoring=\"precision\", \n",
        "    X=X_tfidf, y=y, cv=10)\n",
        "\n",
        "print(\"RF corss validation :  %0.4f accuracy  with a standard deviation of %0.4f\" % (accuracies_rtree_accuracy.mean(), accuracies_rtree_accuracy.std()))\n",
        "print(\"RF corss validation :  %0.4f f1     with a standard deviation of %0.4f\" % (accuracies_rtree_f1.mean(), accuracies_rtree_f1.std()))\n",
        "print(\"RF corss validation :  %0.4f recall   with a standard deviation of %0.4f\" % (accuracies_rtree_recall.mean(), accuracies_rtree_recall.std()))\n",
        "print(\"RF corss validation :  %0.4f precision with a standard deviation of %0.4f\" % (accuracies_rtree_precision.mean(), accuracies_rtree_precision.std()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-0OMq4JE05bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"this is artifacial neural network part\")\n",
        "print(\"=======================================================================================================\")\n",
        "\n",
        "# Part 2 - Now let's make the ANN!\n",
        "X = message_bow_tfidf\n",
        "y = raw_df['label']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler(with_mean=False)\n",
        "\n",
        "\n",
        "X_train_sc = sc.fit(X_train)\n",
        "X_train = X_train_sc.transform(X_train)\n",
        "X_test_sc = sc.fit(X_test)\n",
        "X_test = X_test_sc.transform(X_test)\n",
        "\n",
        "X_train_ann = X_train.copy()\n",
        "X_train_ann = X_train_ann.toarray()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_ann = X.copy()\n",
        "X_ann = X_ann.toarray()\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# Initialising the ANN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu', input_dim = 50358))\n",
        "classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "# Compiling the ANN\n",
        "annn = classifier.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the ANN to the Training set\n",
        "\n",
        "ann = classifier.fit(X_train_ann, y_train, batch_size = 16, epochs = 1, shuffle=False)\n",
        "\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = (classifier.predict(X_test) > 0.5).astype(\"int32\")\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"ann score\",classification_report(y_test,y_pred, digits=4))\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_pred)\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "def create_keras_model():\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu', input_dim = 50358))\n",
        "    model.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    model.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "    rmsprop = RMSprop()\n",
        "  #  rmsprop = RMSprop(learning_rate=0.001)\n",
        "    # Compiling the ANN\n",
        "    model.compile(optimizer = rmsprop, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    model.summary()\n",
        "    # Fitting the ANN to the Training set\n",
        "\n",
        "\n",
        "\n",
        "    # Predicting the Test set results\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "ann2 = KerasClassifier(\n",
        "        build_fn=create_keras_model, \n",
        "        batch_size=16, epochs=15)\n",
        "\n",
        "\n",
        "ann2._estimator_type = \"classifier\" \n",
        "\n",
        "\n",
        "accuracies_ann_accuracy = cross_val_score(estimator=ann2, scoring=\"accuracy\", \n",
        "    X=X_ann, y=y, cv=10)\n",
        "accuracies_ann_f1 = cross_val_score(estimator=ann2, scoring=\"f1\", \n",
        "    X=X_ann, y=y, cv=10)\n",
        "accuracies_ann_recall = cross_val_score(estimator=ann2, scoring=\"recall\", \n",
        "    X=X_ann, y=y, cv=10)\n",
        "accuracies_ann_precision = cross_val_score(estimator=ann2, scoring=\"precision\", \n",
        "    X=X_ann, y=y, cv=10)\n",
        "\n",
        "print(\"ann corss validation :  %0.4f accuracy  with a standard deviation of %0.4f\" % (accuracies_ann_accuracy.mean(), accuracies_ann_accuracy.std()))\n",
        "print(\"ann corss validation :  %0.4f f1     with a standard deviation of %0.4f\" % (accuracies_ann_f1.mean(), accuracies_ann_f1.std()))\n",
        "print(\"ann corss validation :  %0.4f recall   with a standard deviation of %0.4f\" % (accuracies_ann_recall.mean(), accuracies_ann_recall.std()))\n",
        "print(\"ann corss validation :  %0.4f precision with a standard deviation of %0.4f\" % (accuracies_ann_precision.mean(), accuracies_ann_precision.std()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DJurvwXA1wnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "\n",
        "from keras.layers import SimpleRNN, Embedding, Dense, LSTM\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# number of words used as features\n",
        "max_features = 500000\n",
        "# cut off the words after seeing 500 words in each document(email)\n",
        "maxlen = 500\n",
        "\n",
        "\n",
        "# we will use 80% of data as training, 20% as validation data\n",
        "training_samples = int(4994 * .75)\n",
        "validation_samples = int(4994 - training_samples)\n",
        "# sanity check\n",
        "print(len(raw_df['email']) == (training_samples + validation_samples))\n",
        "print(\"The number of training {0}, validation {1} \".format(training_samples, validation_samples))\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(raw_df['email'])\n",
        "sequences = tokenizer.texts_to_sequences(raw_df['email'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Found {0} unique words: \".format(len(word_index)))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "print(\"data shape: \", data.shape)\n",
        "\n",
        "\n",
        "\n",
        "# shuffle data\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = np.asarray(labels)\n",
        "labels = y[indices]\n",
        "\n",
        "\n",
        "texts_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "texts_test = data[training_samples:]\n",
        "y_test = labels[training_samples:]\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "max_features =500000\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.001), loss='binary_crossentropy', metrics=['acc'])\n",
        "history_ltsm = model.fit(texts_train, y_train, epochs=1, batch_size=16, shuffle=False)\n",
        "\n",
        "pred = (model.predict(texts_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "acc = model.evaluate(texts_test, y_test)\n",
        "proba_ltsm = (model.predict(texts_test) > 0.5).astype(\"int32\")\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"Test loss is {0:.4f} accuracy is {1:.4f}  \".format(acc[0],acc[1]))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"LSTM score\",classification_report(y_test,pred, digits=4))\n",
        "print(confusion_matrix(pred, y_test))\n",
        "\n",
        "from pandas.core.common import random_state\n",
        "\n",
        "from keras.layers.core import Flatten\n",
        "\n",
        "\n",
        "\n",
        "def create_keras_model():\n",
        "\n",
        "    max_features =500000\n",
        "    model2 = Sequential()\n",
        "    model2.add(Embedding(max_features, 128))\n",
        "    model2.add(LSTM(128, return_sequences=True))\n",
        "    model2.add(LSTM(128, return_sequences=True))\n",
        "    model2.add(LSTM(128, return_sequences=True))\n",
        "    model2.add(LSTM(128))\n",
        "    model2.add(Dense(128, activation='relu'))\n",
        "    model2.add(Dense(1, activation='sigmoid'))\n",
        "#    rmsprop1 = RMSprop(learning_rate = 0.1)\n",
        "    model2.compile(optimizer=RMSprop(learning_rate=0.001), loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "\n",
        "\n",
        "    return model2\n",
        "\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "LSTMmodel1 = KerasClassifier(\n",
        "        build_fn=create_keras_model, \n",
        "        batch_size=16, epochs=10)\n",
        "\n",
        "\n",
        "LSTMmodel1._estimator_type = \"classifier\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "accuracies_LSTMmodel1_accuracy = cross_val_score(estimator=LSTMmodel1, scoring=\"accuracy\", \n",
        "    X=data, y=labels, cv=10)\n",
        "accuracies_LSTMmodel1_f1 = cross_val_score(estimator=LSTMmodel1, scoring=\"f1\", \n",
        "    X=data, y=labels, cv=10)\n",
        "accuracies_LSTMmodel1_recall = cross_val_score(estimator=LSTMmodel1, scoring=\"recall\", \n",
        "    X=data, y=labels, cv=10)\n",
        "accuracies_LSTMmodel1_precision = cross_val_score(estimator=LSTMmodel1, scoring=\"precision\", \n",
        "    X=data, y=labels, cv=10)\n",
        "\n",
        "print(\"LSTM\")\n",
        "print(\"----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "print(\"LSTMmodel1 corss validation :  %0.4f accuracy  with a standard deviation of %0.4f\" % (accuracies_LSTMmodel1_accuracy.mean(), accuracies_LSTMmodel1_accuracy.std()))\n",
        "print(\"LSTMmodel1 corss validation :  %0.4f f1     with a standard deviation of %0.4f\" % (accuracies_LSTMmodel1_f1.mean(), accuracies_LSTMmodel1_f1.std()))\n",
        "print(\"LSTMmodel1 corss validation :  %0.4f recall   with a standard deviation of %0.4f\" % (accuracies_LSTMmodel1_recall.mean(), accuracies_LSTMmodel1_recall.std()))\n",
        "print(\"LSTMmodel1 corss validation :  %0.4f precision with a standard deviation of %0.4f\" % (accuracies_LSTMmodel1_precision.mean(), accuracies_LSTMmodel1_precision.std()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YxkTMkAx1YEo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}